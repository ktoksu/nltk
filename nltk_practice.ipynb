{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm following the tutorial at:  http://textminingonline.com/dive-into-nltk-part-i-getting-started-with-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_add',\n",
       " '_c2f',\n",
       " '_delimiter',\n",
       " '_encoding',\n",
       " '_f2c',\n",
       " '_file',\n",
       " '_fileids',\n",
       " '_get_root',\n",
       " '_init',\n",
       " '_map',\n",
       " '_para_block_reader',\n",
       " '_pattern',\n",
       " '_resolve',\n",
       " '_root',\n",
       " '_sent_tokenizer',\n",
       " '_sep',\n",
       " '_tagset',\n",
       " '_unload',\n",
       " '_word_tokenizer',\n",
       " 'abspath',\n",
       " 'abspaths',\n",
       " 'categories',\n",
       " 'citation',\n",
       " 'encoding',\n",
       " 'ensure_loaded',\n",
       " 'fileids',\n",
       " 'license',\n",
       " 'open',\n",
       " 'paras',\n",
       " 'raw',\n",
       " 'readme',\n",
       " 'root',\n",
       " 'sents',\n",
       " 'tagged_paras',\n",
       " 'tagged_sents',\n",
       " 'tagged_words',\n",
       " 'unicode_repr',\n",
       " 'words']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# should result in: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
    "brown.words()\n",
    "\n",
    "brown.tagged_words()\n",
    "dir(brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test of concordance with text1 (*Moby Dick*)\n",
    "\n",
    "**Concordance:** function returns every occurrence of a given word, together with some context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 11 of 11 matches:\n",
      "ong the former , one was of a most monstrous size . ... This came towards us , \n",
      "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
      "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
      "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
      "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
      "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
      "th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n",
      "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
      "ere to enter upon those still more monstrous stories of them which are to be fo\n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
      "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test of concordance with *New Yorker* file: pp1-10_utf-8.txt\n",
    "    \n",
    "Very slow!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "don’t know; could see; New York; didn’t know; living room; didn’t\n",
      "want; first time; old man; years ago; don’t want; long time; “Of\n",
      "course; Teacher Fei; Auntie Mei; Super Goat; “I’m sorry; parking lot;\n",
      "even though; don’t think; next day\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.text import Text  \n",
    "\n",
    "f = open('clean\\\\all_utf-8.txt', encoding='utf-8')\n",
    "my_text = f.read()\n",
    "f.close()\n",
    "\n",
    "tokens = nltk.word_tokenize(my_text)\n",
    "text = nltk.Text(tokens)\n",
    "# text.concordance('glamorous')\n",
    "text.collocations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Tests of various functions:\n",
    "\n",
    "**.similar(word, num=20)**: find other words which appear in the same contexts as the specified word; list most similar words first.\n",
    "*Note:* There is a package called *similar_text* that calculates the similarity between two strings.\n",
    "        See: https://pypi.python.org/pypi/similar_text/0.1.3\n",
    "\n",
    "**.similar_words(word, n=20)**\n",
    "Returns similar words\n",
    "\n",
    "**.common_contexts(words, fail_on_unknown=False)**, e.g., text2.common_contexts([\"monstrous\", \"very\"])\n",
    "Find contexts where the specified words can all appear; and return a frequency distribution mapping each context to the number of times that context was used.              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nice in delicious somali jewish native last happens distressing little\n",
      "young wonderful secret only willing year actual on syrup homeless\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import Text  \n",
    "\n",
    "f = open('C:\\\\Users\\\\kt\\\\Desktop\\\\new_yorker_fiction\\\\clean\\\\pp1-10_utf-8.txt')\n",
    "my_text = f.read()\n",
    "f.close()\n",
    "\n",
    "tokens = nltk.word_tokenize(my_text)\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "text.similar('glamorous')\n",
    "# RETURNED: nice in delicious somali jewish native last happens distressing little\n",
    "# young wonderful secret only willing year actual on syrup homeless\n",
    "\n",
    "# text.common_contexts('glamorous', 'style')\n",
    "# RETURNED: ('The following word(s) were not found:', 'g l a m o r o u s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18713),\n",
       " ('the', 13721),\n",
       " ('.', 6862),\n",
       " ('of', 6536),\n",
       " ('and', 6024),\n",
       " ('a', 4569),\n",
       " ('to', 4542),\n",
       " (';', 4072),\n",
       " ('in', 3916),\n",
       " ('that', 2982),\n",
       " (\"'\", 2684),\n",
       " ('-', 2552),\n",
       " ('his', 2459),\n",
       " ('it', 2209),\n",
       " ('I', 2124),\n",
       " ('s', 1739),\n",
       " ('is', 1695),\n",
       " ('he', 1661),\n",
       " ('with', 1659),\n",
       " ('was', 1632),\n",
       " ('as', 1620),\n",
       " ('\"', 1478),\n",
       " ('all', 1462),\n",
       " ('for', 1414),\n",
       " ('this', 1280),\n",
       " ('!', 1269),\n",
       " ('at', 1231),\n",
       " ('by', 1137),\n",
       " ('but', 1113),\n",
       " ('not', 1103),\n",
       " ('--', 1070),\n",
       " ('him', 1058),\n",
       " ('from', 1052),\n",
       " ('be', 1030),\n",
       " ('on', 1005),\n",
       " ('so', 918),\n",
       " ('whale', 906),\n",
       " ('one', 889),\n",
       " ('you', 841),\n",
       " ('had', 767),\n",
       " ('have', 760),\n",
       " ('there', 715),\n",
       " ('But', 705),\n",
       " ('or', 697),\n",
       " ('were', 680),\n",
       " ('now', 646),\n",
       " ('which', 640),\n",
       " ('?', 637),\n",
       " ('me', 627),\n",
       " ('like', 624)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.text import Text  \n",
    "\n",
    "# frequency distribution\n",
    "fdist1 = FreqDist(text1)\n",
    "fdist1.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "f = open('clean\\\\pp1-10.txt')\n",
    "my_text = f.read()\n",
    "f.close()\n",
    "\n",
    "# bigrams\n",
    "# tokens = nltk.word_tokenize(my_text)\n",
    "# list(nltk.bigrams(tokens))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "print(result)\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auntie Mei; don’t know; New York; Father Paul; Big Uncle; Pending\n",
      "Vegan; don’t want; Mr. Oakes; living room; Boris Ivanovich; didn’t\n",
      "want; didn’t know; could see; Ted Martin; St. Savior; Aunt Cissy; long\n",
      "time; first time; Ibn Rushd; Sister St.\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "f = open('clean\\\\pp1-10_utf-8.txt')\n",
    "my_text = f.read()\n",
    "f.close()\n",
    "\n",
    "# use to rejoin list elements into string:  ' '.join(tokens) \n",
    "\n",
    "# tokens = word_tokenize(my_text)\n",
    "# text = nltk.Text(tokens)  # splits text into a list of words\n",
    "\n",
    "'''COLLOCATIONS: expressions of multiple words which commonly co-occur: the below returns:\n",
    "Auntie Mei; don’t know; New York; Father Paul; Big Uncle; Pending\n",
    "Vegan; don’t want; Mr. Oakes; living room; Boris Ivanovich; didn’t\n",
    "want; didn’t know; could see; Ted Martin; St. Savior; Aunt Cissy; long\n",
    "time; first time; Ibn Rushd; Sister St.\n",
    "'''\n",
    "# text.collocations()\n",
    "'''The below regex searches may be useful to check whether a trigram is proper English, makes sense.\n",
    "'''\n",
    "# text.findall(r\"<a> (<.*>) <man>\")\n",
    "# text.findall(r\"<across> <the> (<.*>) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNP',\n",
       " 'NNP',\n",
       " 'VBD',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'VBN',\n",
       " 'IN',\n",
       " 'DT',\n",
       " 'NNS',\n",
       " ',',\n",
       " 'WDT',\n",
       " 'PRP',\n",
       " 'VBD',\n",
       " 'VBN',\n",
       " 'TO',\n",
       " 'PRP$',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " 'IN',\n",
       " 'NNP',\n",
       " 'NNP',\n",
       " ',',\n",
       " 'DT',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'VBN',\n",
       " 'TO',\n",
       " 'NNP',\n",
       " 'VBG',\n",
       " 'JJ',\n",
       " 'NN',\n",
       " '.']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sent_tokenize(my_text)  # splits text into a list of sentences\n",
    "# pprint.pprint(sents[0:2])  # returns sentences 1 and 2\n",
    "\n",
    "'''Get the first sentence, tokenize that sentence, then POS tag it.'''\n",
    "sent1 = sents[0]\n",
    "sent1_tokens = word_tokenize(sent1)\n",
    "sent1_pos_tags = nltk.pos_tag(sent1_tokens)\n",
    "# sent1_pos_tags[0]  # returns:  ('Mrs.', 'NNP')\n",
    "# sent1_pos_tags[0][1]  # returns:  'NNP'\n",
    "\n",
    "'''Create list of POS of that first sentence'''\n",
    "pos_list = []\n",
    "# for i in range(len(sent1_pos_tags)):\n",
    "for i in range(13):\n",
    "    pos_list.append(sent1_pos_tags[i][1])\n",
    "\n",
    "'''To pickle all tagged text, see:  http://www.nltk.org/book/ch05.html'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
